{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greeshmakarthesha-jpg/STP_projects_assignments/blob/main/Copy_of_STP_Module_3_project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "unG213zJhFsx"
      },
      "source": [
        "# **Student Training Program on AIML**\n",
        "# Project : Data Visualization, Choosing K-value and Appreciating Feature Scaling and Standardization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "1YbIjlgrhFsz"
      },
      "source": [
        "## Binary Classification Task: Diabetes Dataset\n",
        "\n",
        "We'll be using ML techniques learnt uptil now to predict whether a Pima Indian Woman has diabetes or not, based on information about the patient such as blood pressure, body mass index (BMI), age, etc.\n",
        "\n",
        "**Dataset Source:** [Pima Indians Diabetes Database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcQGSOFchFs0"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Scientists carried out a study to investigate the significance of health-related predictors of diabetes in **Pima Indian Women**. The study population was females (21 years and above) of Pima Indian heritage.\n",
        "\n",
        "The purpose of the study was to find out the factors that are associated with the presence of diabetes in Pima Indians.\n",
        "\n",
        "To find out the reason behind this, we have to first analyze the relationship between different features, such as the number of times a woman was pregnant, their BMI, prevalence of diabetes, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQIjzjdvhFs1"
      },
      "source": [
        "## Exploratory Data Analysis (EDA) and Statistical Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBuD7M7yhFs1"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-61IRwW9hFs2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsuSvHBhFs2"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "Upload the diabetes.csv file that has been provided to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su9YeUtjx7Vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "ead0f978-9710-4820-b183-9c4fb80359f9"
      },
      "source": [
        "# Upload the diabetes data CSV file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bf8bfc9f-32b4-43b4-8ac2-02b590a0e5bd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bf8bfc9f-32b4-43b4-8ac2-02b590a0e5bd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WlW8jTBChFs3"
      },
      "source": [
        "# Load the dataset\n",
        "diabetes_data = pd.read_csv('diabetes.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4u_B2qrhFs3"
      },
      "source": [
        "## Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OEjQIGryhFs3"
      },
      "source": [
        "# Display the first few rows\n",
        "diabetes_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PfZJ_z6lhFs3"
      },
      "source": [
        "# Check dataset dimensions\n",
        "print(f\"Dataset shape: {diabetes_data.shape}\")\n",
        "print(f\"Number of samples: {diabetes_data.shape[0]}\")\n",
        "print(f\"Number of features: {diabetes_data.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6mHTR0vthFs4"
      },
      "source": [
        "# Dataset information\n",
        "diabetes_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bAj3i_HwhFs4"
      },
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(diabetes_data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N6CuF5PhFs4"
      },
      "source": [
        "## Statistical Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6d3ZtzAahFs4"
      },
      "source": [
        "# Descriptive statistics\n",
        "diabetes_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o75MYhOlhFs5"
      },
      "source": [
        "# Check the target variable distribution\n",
        "print(\"Outcome distribution:\")\n",
        "print(diabetes_data['Outcome'].value_counts())\n",
        "print(f\"\\nPercentage of diabetic cases: {diabetes_data['Outcome'].mean()*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcXm3l97hFs5"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kQ_wBuZxhFs5"
      },
      "source": [
        "# Visualize the target variable distribution\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(x='Outcome', data=diabetes_data, palette='viridis')\n",
        "plt.title('Distribution of Diabetes Outcome', fontsize=14)\n",
        "plt.xlabel('Outcome (0: No Diabetes, 1: Diabetes)', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FjVuHRx_hFs5"
      },
      "source": [
        "# Distribution of all features\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(diabetes_data.columns):\n",
        "    sns.histplot(diabetes_data[col], kde=True, ax=axes[i], color='steelblue')\n",
        "    axes[i].set_title(f'Distribution of {col}', fontsize=12)\n",
        "    axes[i].set_xlabel(col, fontsize=10)\n",
        "    axes[i].set_ylabel('Frequency', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDYg1z7XhFs6"
      },
      "source": [
        "## TASK-1: Correlation Analysis\n",
        "\n",
        "Compute the correlation matrix and visualize it using a heatmap to understand the relationships between different features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your dataset is in a pandas DataFrame called df\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u6TkSzo9dJ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C2TLmHGThFs6"
      },
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = diabetes_data.corr()\n",
        "\n",
        "# Visualize with heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Feature Correlation Heatmap', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q94r-_jghFs6"
      },
      "source": [
        "## TASK-2: Comparative Box Plots\n",
        "\n",
        "Create box plots to compare the distribution of each feature for diabetic vs non-diabetic patients."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = df.columns.drop('Outcome')\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, feature in enumerate(features, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(x='Outcome', y=feature, data=df)\n",
        "    plt.title(feature)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jkHC-OYhdO50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7sAO3cLGhFs6"
      },
      "source": [
        "# Box plots for all features grouped by Outcome\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "feature_cols = diabetes_data.columns[:-1]  # All columns except Outcome\n",
        "\n",
        "for i, col in enumerate(feature_cols):\n",
        "    sns.boxplot(x='Outcome', y=col, data=diabetes_data, ax=axes[i], palette='Set2')\n",
        "    axes[i].set_title(f'{col} by Diabetes Outcome', fontsize=12)\n",
        "    axes[i].set_xlabel('Outcome (0: No, 1: Yes)', fontsize=10)\n",
        "    axes[i].set_ylabel(col, fontsize=10)\n",
        "\n",
        "# Hide the last subplot if not needed\n",
        "axes[-1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i8EL2K7hFs7"
      },
      "source": [
        "## TASK-3: Pairplot Analysis\n",
        "\n",
        "Create a pairplot to visualize the pairwise relationships between features, colored by the Outcome variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(df, hue='Outcome', diag_kind='kde', corner=True)\n",
        "plt.suptitle(\"Pairplot of Features Colored by Outcome\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mkWAtJ3AdVfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yWXt87N6hFs7"
      },
      "source": [
        "# Select a subset of features for clearer visualization\n",
        "selected_features = ['Glucose', 'BMI', 'Age', 'Insulin', 'Outcome']\n",
        "sns.pairplot(diabetes_data[selected_features], hue='Outcome', palette='husl', diag_kind='kde', height=2.5)\n",
        "plt.suptitle('Pairplot of Selected Features', y=1.02, fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaHxg3zChFs7"
      },
      "source": [
        "## K-Nearest Neighbors (KNN) Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gHUK5LO6hFs7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CWv_3k1hFs8"
      },
      "source": [
        "## TASK-4: Train-Test Split\n",
        "\n",
        "Split the dataset into training (70%) and testing (30%) sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "N0pT3VL8dfX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XqENgvmyhFs8"
      },
      "source": [
        "# Prepare features and target\n",
        "X = diabetes_data.drop('Outcome', axis=1)\n",
        "y = diabetes_data['Outcome']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE82jWpghFs8"
      },
      "source": [
        "## TASK-5: Build and Evaluate KNN Model\n",
        "\n",
        "Train a KNN classifier with k=1 and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train KNN with k = 1\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1t3RnNowdna3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1LB7_CeXhFs8"
      },
      "source": [
        "# Train KNN with k=1\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"\\nAccuracy Score: {accuracy_score(y_test, y_pred)*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9d87PlNhFs9"
      },
      "source": [
        "## TASK-6: Finding the Optimal K Value\n",
        "\n",
        "Test different values of K (from 1 to 40) and plot the error rate to find the optimal K value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "error_rate = []\n",
        "\n",
        "# Try K values from 1 to 40\n",
        "for k in range(1, 41):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    error = np.mean(y_pred != y_test)\n",
        "    error_rate.append(error)\n",
        "\n",
        "# Plot error rate vs K\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, 41), error_rate, marker='o')\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Error Rate\")\n",
        "plt.title(\"Error Rate vs K in KNN\")\n",
        "plt.xticks(range(1, 41, 2))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i_tqnrWxd0FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "s5TDYd1ChFs9"
      },
      "source": [
        "# Store the original unscaled data for later comparison\n",
        "unchanged_data = X.copy()\n",
        "\n",
        "# Function to evaluate KNN performance across different K values\n",
        "def plot_KNN_error_rate(xdata, ydata):\n",
        "    error_rate = []\n",
        "    test_scores = []\n",
        "    train_scores = []\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(xdata, ydata, test_size=0.3, random_state=101)\n",
        "\n",
        "    for i in range(1, 40):\n",
        "        knn = KNeighborsClassifier(n_neighbors=i)\n",
        "        knn.fit(X_train, y_train)\n",
        "        pred_i = knn.predict(X_test)\n",
        "\n",
        "        error_rate.append(np.mean(pred_i != y_test))\n",
        "        train_scores.append(knn.score(X_train, y_train))\n",
        "        test_scores.append(knn.score(X_test, y_test))\n",
        "\n",
        "    # Plot error rate\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(range(1, 40), error_rate, color='blue', linestyle='dashed', marker='o',\n",
        "             markerfacecolor='red', markersize=8)\n",
        "    plt.title('Error Rate vs. K Value', fontsize=14)\n",
        "    plt.xlabel('K', fontsize=12)\n",
        "    plt.ylabel('Error Rate', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Find best K values\n",
        "    max_train_score = max(train_scores)\n",
        "    train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\n",
        "    print(f'Max train score: {max_train_score*100:.2f}% at K = {[x+1 for x in train_scores_ind]}')\n",
        "\n",
        "    max_test_score = max(test_scores)\n",
        "    test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\n",
        "    print(f'Max test score: {max_test_score*100:.2f}% at K = {[x+1 for x in test_scores_ind]}')\n",
        "\n",
        "    return test_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ADZBCgEdhFs9"
      },
      "source": [
        "# Evaluate on unscaled data\n",
        "print(\"Performance on Unscaled Data:\")\n",
        "unchanged_test_scores = plot_KNN_error_rate(unchanged_data, diabetes_data['Outcome'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OZFsqr1hFtE"
      },
      "source": [
        "## Standardize the Variables\n",
        "\n",
        "Standardization (also called z-score normalization) is the process of putting different variables on the same scale. Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "$$ Z = {X - \\mu \\over \\sigma}$$\n",
        "\n",
        "Where:\n",
        "- Z is the standardized value\n",
        "- X is the original value\n",
        "- μ is the mean of the feature\n",
        "- σ is the standard deviation of the feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kgZdXekchFtE"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on training data features\n",
        "scaler.fit(diabetes_data.drop('Outcome', axis=1))\n",
        "\n",
        "# Transform the features\n",
        "scaled_data = scaler.transform(diabetes_data.drop('Outcome', axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OFANLGLuhFtF"
      },
      "source": [
        "# Create a dataframe with scaled features\n",
        "df_feat = pd.DataFrame(scaled_data, columns=diabetes_data.columns[:-1])\n",
        "df_feat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w_9Qm7LKhFtF"
      },
      "source": [
        "# Verify standardization: mean should be ~0, std should be ~1\n",
        "print(\"Mean of scaled features:\")\n",
        "print(df_feat.mean())\n",
        "print(\"\\nStandard deviation of scaled features:\")\n",
        "print(df_feat.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fwuhzRarQRRQ"
      },
      "source": [
        "# Evaluate on scaled data\n",
        "print(\"Performance on Standardized Data:\")\n",
        "scaled_test_scores = plot_KNN_error_rate(scaled_data, diabetes_data['Outcome'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoBBRF8FRkrV"
      },
      "source": [
        "## Comparing Accuracy Before and After Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M9uVwaaBQ7RE"
      },
      "source": [
        "# Compare performance\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(range(1, 40), unchanged_test_scores, marker='o', label='Unscaled Data', linewidth=2)\n",
        "plt.plot(range(1, 40), scaled_test_scores, marker='s', label='Standardized Data', linewidth=2)\n",
        "plt.title('KNN Accuracy: Unscaled vs Standardized Data', fontsize=14)\n",
        "plt.xlabel('K Value', fontsize=12)\n",
        "plt.ylabel('Test Accuracy', fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj2xeXZthFtM"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "From the data analysis we carried out, it seems that there is some form of an association between BMI, number of pregnancies, glucose levels, and the test results for diabetes.\n",
        "\n",
        "As for the classification tasks, the standardized data yields much better results than the unscaled data over most of the K-values considered, thus indicating the importance of standardizing data in Machine Learning problems. This improvement occurs because:\n",
        "\n",
        "1. KNN is distance-based and features with larger scales dominate the distance calculation\n",
        "2. Standardization puts all features on equal footing\n",
        "3. This leads to more balanced and accurate predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "challenge_section"
      },
      "source": [
        "---\n",
        "# DIY Challenges\n",
        "\n",
        "Test your understanding by completing these three challenges. Solutions are not provided - work through them independently!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "challenge_1"
      },
      "source": [
        "## Challenge 1: MinMax Scaling Comparison\n",
        "\n",
        "**Objective:** Implement MinMax scaling and compare its performance with StandardScaler.\n",
        "\n",
        "**Tasks:**\n",
        "1. Import `MinMaxScaler` from sklearn.preprocessing\n",
        "2. Apply MinMax scaling to the dataset (scales features to range [0, 1])\n",
        "3. Use the `plot_KNN_error_rate()` function on the MinMax-scaled data\n",
        "4. Create a comparison plot showing all three approaches: unscaled, standardized, and MinMax-scaled\n",
        "5. Analyze which scaling method performs best and why\n",
        "\n",
        "**Hint:** MinMaxScaler formula is: $X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "challenge_1_code"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "# MinMax Scaling\n",
        "minmax_scaler = MinMaxScaler()\n",
        "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
        "X_test_minmax = minmax_scaler.transform(X_test)\n",
        "error_minmax = plot_KNN_error_rate(\n",
        "    X_train_minmax, X_test_minmax, y_train, y_test\n",
        ")\n",
        "# Standard Scaling\n",
        "std_scaler = StandardScaler()\n",
        "X_train_std = std_scaler.fit_transform(X_train)\n",
        "X_test_std = std_scaler.transform(X_test)\n",
        "\n",
        "error_unscaled = plot_KNN_error_rate(X_train, X_test, y_train, y_test)\n",
        "error_std = plot_KNN_error_rate(X_train_std, X_test_std, y_train, y_test)\n",
        "\n",
        "# Plot comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k_values = range(1, len(error_unscaled) + 1)\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(k_values, error_unscaled, label=\"Unscaled\", marker='o')\n",
        "plt.plot(k_values, error_std, label=\"StandardScaler\", marker='o')\n",
        "plt.plot(k_values, error_minmax, label=\"MinMaxScaler\", marker='o')\n",
        "\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Error Rate\")\n",
        "plt.title(\"KNN Error Rate Comparison for Different Scaling Methods\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "challenge_2"
      },
      "source": [
        "## Challenge 2: K-Fold Cross-Validation Analysis\n",
        "\n",
        "**Objective:** Implement K-Fold cross-validation to get more robust performance estimates.\n",
        "\n",
        "**Tasks:**\n",
        "1. Import `cross_val_score` from sklearn.model_selection\n",
        "2. Implement 5-fold cross-validation for KNN with K values from 1 to 40\n",
        "3. Compare cross-validation scores for:\n",
        "   - Unscaled data\n",
        "   - Standardized data\n",
        "   - MinMax-scaled data (from Challenge 1)\n",
        "4. Plot the mean cross-validation accuracy with standard deviation error bars\n",
        "5. Determine the optimal K value for each scaling approach\n",
        "\n",
        "**Bonus:** Calculate and display the standard deviation of cross-validation scores to understand model stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "challenge_2_code"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "# Unscaled\n",
        "X_unscaled = X\n",
        "\n",
        "# Standardized\n",
        "std_scaler = StandardScaler()\n",
        "X_std = std_scaler.fit_transform(X)\n",
        "\n",
        "# MinMax scaled\n",
        "minmax_scaler = MinMaxScaler()\n",
        "X_minmax = minmax_scaler.fit_transform(X)\n",
        "k_values = range(1, 41)\n",
        "\n",
        "def knn_cv_results(X, y):\n",
        "    mean_scores = []\n",
        "    std_scores = []\n",
        "\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
        "        mean_scores.append(scores.mean())\n",
        "        std_scores.append(scores.std())\n",
        "\n",
        "    return np.array(mean_scores), np.array(std_scores)\n",
        "\n",
        "mean_unscaled, std_unscaled = knn_cv_results(X_unscaled, y)\n",
        "mean_std, std_std = knn_cv_results(X_std, y)\n",
        "mean_minmax, std_minmax = knn_cv_results(X_minmax, y)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.errorbar(k_values, mean_unscaled, yerr=std_unscaled,\n",
        "             label=\"Unscaled\", marker='o', capsize=3)\n",
        "\n",
        "plt.errorbar(k_values, mean_std, yerr=std_std,\n",
        "             label=\"StandardScaler\", marker='o', capsize=3)\n",
        "\n",
        "plt.errorbar(k_values, mean_minmax, yerr=std_minmax,\n",
        "             label=\"MinMaxScaler\", marker='o', capsize=3)\n",
        "\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.title(\"5-Fold Cross-Validation Accuracy vs K\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(\"Optimal K (Unscaled):\", k_values[np.argmax(mean_unscaled)])\n",
        "print(\"Optimal K (StandardScaler):\", k_values[np.argmax(mean_std)])\n",
        "print(\"Optimal K (MinMaxScaler):\", k_values[np.argmax(mean_minmax)])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "challenge_3"
      },
      "source": [
        "## Challenge 3: Feature Selection and Performance Analysis\n",
        "\n",
        "**Objective:** Investigate how feature selection impacts KNN performance.\n",
        "\n",
        "**Tasks:**\n",
        "1. Based on the correlation analysis from TASK-1, identify the top 4 features most correlated with 'Outcome'\n",
        "2. Create a reduced dataset with only these 4 features\n",
        "3. Apply standardization to this reduced feature set\n",
        "4. Train KNN models (K from 1 to 40) on both:\n",
        "   - Full feature set (standardized)\n",
        "   - Reduced feature set (standardized)\n",
        "5. Create a comparison plot showing:\n",
        "   - Accuracy vs K for full features\n",
        "   - Accuracy vs K for reduced features\n",
        "6. Analyze:\n",
        "   - Does reducing features improve or hurt performance?\n",
        "   - What is the optimal K for each feature set?\n",
        "   - What insights can you draw about feature importance?\n",
        "\n",
        "**Bonus:** Try different numbers of features (3, 5, 6) and see how it affects performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "challenge_3_code"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Compute correlation with Outcome\n",
        "corr_matrix = df.corr()\n",
        "corr_with_target = corr_matrix['Outcome'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Top 4 features (excluding Outcome itself)\n",
        "top_features = corr_with_target.index[1:5].tolist()\n",
        "print(\"Top 4 features:\", top_features)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features and target\n",
        "X_full = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome']\n",
        "\n",
        "# Reduced dataset with top 4 features\n",
        "X_reduced = df[top_features]\n",
        "\n",
        "# Standardize\n",
        "scaler_full = StandardScaler()\n",
        "X_full_std = scaler_full.fit_transform(X_full)\n",
        "\n",
        "scaler_reduced = StandardScaler()\n",
        "X_reduced_std = scaler_reduced.fit_transform(X_reduced)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "k_values = range(1, 41)\n",
        "\n",
        "def knn_cv_accuracy(X, y):\n",
        "    mean_acc = []\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
        "        mean_acc.append(scores.mean())\n",
        "    return mean_acc\n",
        "\n",
        "# CV accuracy\n",
        "accuracy_full = knn_cv_accuracy(X_full_std, y)\n",
        "accuracy_reduced = knn_cv_accuracy(X_reduced_std, y)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(k_values, accuracy_full, label=\"Full Features\", marker='o')\n",
        "plt.plot(k_values, accuracy_reduced, label=\"Top 4 Features\", marker='o')\n",
        "\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.title(\"KNN Accuracy: Full vs Reduced Feature Set\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "optimal_k_full = k_values[np.argmax(accuracy_full)]\n",
        "optimal_k_reduced = k_values[np.argmax(accuracy_reduced)]\n",
        "\n",
        "print(\"Optimal K (Full Features):\", optimal_k_full)\n",
        "print(\"Optimal K (Top 4 Features):\", optimal_k_reduced)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aubGhsTSjJLM"
      },
      "source": [
        "---\n",
        "# References\n",
        "\n",
        "1. [Pima Indians Diabetes Database - Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)\n",
        "2. [Scikit-learn Documentation - KNN Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "3. [Scikit-learn Documentation - StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "4. [Scikit-learn Documentation - MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    }
  ]
}